{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Bootstrap based on Kline and Santos (2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ___  ____  ____  ____  ____ ®\n",
      " /__    /   ____/   /   ____/      Stata 18.0\n",
      "___/   /   /___/   /   /___/       MP—Parallel Edition\n",
      "\n",
      " Statistics and Data Science       Copyright 1985-2023 StataCorp LLC\n",
      "                                   StataCorp\n",
      "                                   4905 Lakeway Drive\n",
      "                                   College Station, Texas 77845 USA\n",
      "                                   800-782-8272        https://www.stata.com\n",
      "                                   979-696-4600        service@stata.com\n",
      "\n",
      "Stata license: Single-user 2-core  perpetual\n",
      "Serial number: 501806367191\n",
      "  Licensed to: Aleksandr Michuda\n",
      "               Cornell University\n",
      "\n",
      "Notes:\n",
      "      1. Unicode is supported; see help unicode_advice.\n",
      "      2. More than 2 billion observations are allowed; see help obs_advice.\n",
      "      3. Maximum number of variables is set to 5,000 but can be increased;\n",
      "          see help set_maxvar.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from wildboottest.wildboottest import wildboottest\n",
    "from wildboottest.weights import draw_weights\n",
    "from statsmodels.discrete.discrete_model import Probit\n",
    "from numba import prange, jit\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import stata_setup\n",
    "stata_setup.config(path = \"/Applications/Stata\", \n",
    "                   edition = \"mp\")\n",
    "from pystata import stata\n",
    "import sfi\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stata Code from Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". \n",
      ". ****************************************************\n",
      ". *  Code for Table 2 of Kline and Santos (2011)     *\n",
      ". *  This file computes Analytical Wald and LM tests *\n",
      ". *  along with Score bootstrapped tests.            *\n",
      ". *  Some other results not reported in the paper    *\n",
      ". *  are also included.                              *    \n",
      ". ****************************************************\n",
      ". \n",
      ". version 11.1\n",
      "\n",
      ". set seed 12345\n",
      "\n",
      ". \n",
      ". **mata subroutine to compute recentered clustered outer products**\n",
      ". **inputs a matrix S of scores and a cluster summing matrix L**\n",
      ". **outputs clustered outer product matrix**\n",
      ". **note this program assumes a balanced design**\n",
      ". **and proper sort order**\n",
      ". cap mata: mata drop clustOPG()\n",
      "\n",
      ". mata: \n",
      "------------------------------------------------- mata (type end to exit) -----\n",
      ": real matrix clustOPG(real matrix S,real matrix L){\n",
      "> real scalar N\n",
      "> real matrix Scent, Scentsum, OPGcent\n",
      "> \n",
      "> N=rows(S)\n",
      "> Scent=S - J(N,1,1)*mean(S)\n",
      "> Scentsum=L'*Scent\n",
      "> OPGcent=Scentsum'*Scentsum\n",
      "> return(OPGcent)\n",
      "> }\n",
      "\n",
      ": end\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      ". \n",
      ". \n",
      ". \n",
      ". /* cap prog drop sim_probit\n",
      "> prog def sim_probit, rclass\n",
      "> syntax [, c(integer 5) m(integer 50) f(integer 0) r(integer 199)] */\n",
      ". *Notes on syntax: \n",
      ". *c - # of clusters\n",
      ". *m - # of obs per cluster\n",
      ". *f - controls whether to use mixture regressor\n",
      ". *r - # of bootstrap reps per simulation\n",
      ". \n",
      ". local c 10\n",
      "\n",
      ". local m 100\n",
      "\n",
      ". local f 0\n",
      "\n",
      ". local r 1000\n",
      "\n",
      ". \n",
      ". **Generate data**\n",
      ". drop _all\n",
      "\n",
      ". set obs `c'  //generate c clusters\n",
      "Number of observations (_N) was 0, now 10.\n",
      "\n",
      ". \n",
      ". gen x=invnorm(uniform()) //covariate\n",
      "\n",
      ". gen mix=uniform()>.1 //contamination probability\n",
      "\n",
      ". \n",
      ". if `f'==1{\n",
      ".         gen D=(mix*invnorm(uniform()) + (1-mix)*(2+3*invnorm(uniform())))/4 /\n",
      "> /regressor of interest\n",
      ". }\n",
      "\n",
      ". else{\n",
      ".         gen D=(invnorm(uniform()))/4 //regressor of interest\n",
      ". }\n",
      "\n",
      ". \n",
      ". gen v=invnorm(uniform())\n",
      "\n",
      ". \n",
      ". \n",
      ". gen c=_n        //generate cluster id\n",
      "\n",
      ". \n",
      ". expand `m'      //make m obs per cluster\n",
      "(990 observations created)\n",
      "\n",
      ". \n",
      ". replace x=(x+invnorm(uniform()))/4  //add some within cluster variation in x\n",
      "(1,000 real changes made)\n",
      "\n",
      ". \n",
      ". gen y=x+D+v/sqrt(2)+invnorm(uniform())/sqrt(2)  //form outcome\n",
      "\n",
      ". replace y=y>0\n",
      "(1,000 real changes made)\n",
      "\n",
      ". sort c\n",
      "\n",
      ". \n",
      ". **Restricted**\n",
      ". constraint 1 D=1\n",
      "\n",
      ". glm y x D, constraint(1) fam(bin) link(probit)\n",
      "\n",
      "Iteration 0:  Log likelihood = -685.70942  \n",
      "Iteration 1:  Log likelihood = -685.44941  \n",
      "Iteration 2:  Log likelihood =  -685.4494  \n",
      "\n",
      "Generalized linear models                         Number of obs   =      1,000\n",
      "Optimization     : ML                             Residual df     =        998\n",
      "                                                  Scale parameter =          1\n",
      "Deviance         =  1370.898802                   (1/df) Deviance =   1.373646\n",
      "Pearson          =  1097.020519                   (1/df) Pearson  =   1.099219\n",
      "\n",
      "Variance function: V(u) = u*(1-u)                 [Bernoulli]\n",
      "Link function    : g(u) = invnorm(u)              [Probit]\n",
      "\n",
      "                                                  AIC             =   1.374899\n",
      "Log likelihood   = -685.4494008                   BIC             =  -5523.041\n",
      "\n",
      " ( 1)  [y]D = 1\n",
      "------------------------------------------------------------------------------\n",
      "             |                 OIM\n",
      "           y | Coefficient  std. err.      z    P>|z|     [95% conf. interval]\n",
      "-------------+----------------------------------------------------------------\n",
      "           x |   .9846371   .1474927     6.68   0.000     .6955568    1.273717\n",
      "           D |          1  (constrained)\n",
      "       _cons |   .1066109    .040653     2.62   0.009     .0269325    .1862893\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      ". predict pr\n",
      "(option mu assumed; predicted mean y)\n",
      "\n",
      ". predict xbr, xb\n",
      "\n",
      ". gen phir=normalden(xbr)\n",
      "\n",
      ". gen er=(y-pr)*phir/(pr*(1-pr)) //quasi-residual\n",
      "\n",
      ". gen wr=phir^2/(pr*(1-pr)) //weights for hessian\n",
      "\n",
      ". gen one=1\n",
      "\n",
      ". \n",
      ". mat accum Hr=one x D [iw=wr], nocons //form hessian */\n",
      "(obs=598.5860322)\n",
      "\n",
      ". \n",
      ". \n",
      ". **Unrestricted**\n",
      ". probit y x D, cluster(c)\n",
      "\n",
      "Iteration 0:  Log pseudolikelihood = -693.12918  \n",
      "Iteration 1:  Log pseudolikelihood =  -665.3805  \n",
      "Iteration 2:  Log pseudolikelihood = -665.33497  \n",
      "Iteration 3:  Log pseudolikelihood = -665.33497  \n",
      "\n",
      "Probit regression                                       Number of obs =  1,000\n",
      "                                                        Wald chi2(2)  =   7.41\n",
      "                                                        Prob > chi2   = 0.0247\n",
      "Log pseudolikelihood = -665.33497                       Pseudo R2     = 0.0401\n",
      "\n",
      "                                     (Std. err. adjusted for 10 clusters in c)\n",
      "------------------------------------------------------------------------------\n",
      "             |               Robust\n",
      "           y | Coefficient  std. err.      z    P>|z|     [95% conf. interval]\n",
      "-------------+----------------------------------------------------------------\n",
      "           x |   1.057877   .3958503     2.67   0.008     .2820252     1.83373\n",
      "           D |   .0942995   1.041973     0.09   0.928    -1.947931     2.13653\n",
      "       _cons |   .0357662   .3079585     0.12   0.908    -.5678212    .6393537\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      ". global b=_b[D]\n",
      "\n",
      ". local stdof=sqrt((`m'*`c'-3)/(`m'*`c'-1)) //inverse stata DOF correction\n",
      "\n",
      ". global W=((_b[D]-1)/(_se[D]*`stdof'))^2 //save analytical Wald\n",
      "\n",
      ". \n",
      ". predict pu\n",
      "(option pr assumed; Pr(y))\n",
      "\n",
      ". predict xbu, xb\n",
      "\n",
      ". gen phiu=normalden(xbu)\n",
      "\n",
      ". gen eu=(y-pu)*phiu/(pu*(1-pu)) //quasi-residual\n",
      "\n",
      ". gen wu=phiu^2/(pu*(1-pu)) //weights for hessian\n",
      "\n",
      ". \n",
      ". local dfk=(`c'/(`c'-1))\n",
      "\n",
      ". mata: L=I(`c')#J(`m',1,1) //matrix for summing across clusters\n",
      "\n",
      ". mata: X=st_data(.,(\"one\", \"x\", \"D\"))\n",
      "\n",
      ". mata: C=(0\\0\\1)         //constraint matrix -- third coefficient is restricte\n",
      "> d\n",
      "\n",
      ". \n",
      ". mata: Hr=st_matrix(\"Hr\")\n",
      "\n",
      ". mata: Hrinv=invsym(Hr)\n",
      "\n",
      ". mata: Ar=C'*Hrinv               //form A_n\n",
      "\n",
      ". \n",
      ". \n",
      ". **Prepare influence function wald\n",
      ". mat accum Hu=one x D [iw=wu], nocons //form hessian\n",
      "(obs=615.9930729)\n",
      "\n",
      ". mata: Hu=st_matrix(\"Hu\")\n",
      "\n",
      ". mata: Huinv=invsym(Hu)\n",
      "\n",
      ". mata: Au=C'*Huinv\n",
      "\n",
      ". mata: eu=st_data(.,\"eu\")\n",
      "\n",
      ". mata: OPGucent=clustOPG(X:*eu,L)\n",
      "\n",
      ". mata: Vinvu=invsym(Au*OPGucent*`dfk'*Au') //compute inverse of variance of in\n",
      "> fluence function\n",
      "\n",
      ". \n",
      ". \n",
      ". save \"sim_sample.dta\", replace\n",
      "file sim_sample.dta saved\n",
      "\n",
      ". \n",
      ". mata: S = X:*eu\n",
      "\n",
      ". \n",
      ". **Bootstrap**\n",
      ". gen u=.\n",
      "(1,000 missing values generated)\n",
      "\n",
      ". gen ystar=.\n",
      "(1,000 missing values generated)\n",
      "\n",
      ". gen ernew=.\n",
      "(1,000 missing values generated)\n",
      "\n",
      ". gen eunew=.\n",
      "(1,000 missing values generated)\n",
      "\n",
      ". gen pos=.\n",
      "(1,000 missing values generated)\n",
      "\n",
      ". \n",
      ". \n",
      ". mat Ts=J(1,6,.) //store bootstrap statistics\n",
      "\n",
      ". \n",
      ". by c: replace u=uniform()\n",
      "(1,000 real changes made)\n",
      "\n",
      ". by c: replace pos=u[1]<.5  //cluster level rademacher indicator\n",
      "(1000 real changes made)\n",
      "\n",
      ". gen radem = 2*pos - 1\n",
      "\n",
      ". \n",
      ". qui replace ernew=(2*pos-1)*er  //weighted residual\n",
      "\n",
      ". qui replace eunew=(2*pos-1)*eu  //weighted residual\n",
      "\n",
      ". \n",
      ". *Score bootstrap LM -- Rademacher\n",
      ". mata: er=st_data(.,\"ernew\")\n",
      "\n",
      ". mata: Sr=colsum(X:*er)\n",
      "\n",
      ". mata: OPGrcent=clustOPG(X:*er,L)\n",
      "\n",
      ". mata: lm=Sr*Ar'*invsym(Ar*(OPGrcent)*`dfk'*Ar')*Ar*Sr'\n",
      "\n",
      ". mata: st_numscalar(\"lm\",lm)\n",
      "\n",
      ". mat Ts[1,1]=lm       //save bootstrap LM \n",
      "\n",
      ". \n",
      ". \n",
      ". *Score Wald -- Rademacher\n",
      ". mata: eu=st_data(.,\"eunew\")\n",
      "\n",
      ". mata: Su=colsum(X:*eu)\n",
      "\n",
      ". mata: OPGucent=clustOPG(X:*eu,L)\n",
      "\n",
      ". mata: wald=Su*Au'*invsym(Au*OPGucent*`dfk'*Au')*Au*Su'\n",
      "\n",
      ". mata: st_numscalar(\"wald\",wald)\n",
      "\n",
      ". mat Ts[1,2]=wald             //save bootstrap Wald\n",
      "\n",
      ". \n",
      ". *Score bootstrap Wald combining restricted and unrestricted scores -- Rademac\n",
      "> her\n",
      ". mata: wald_r=Sr*Ar'*invsym(Au*OPGucent*`dfk'*Au')*Ar*Sr'\n",
      "\n",
      ". mata: st_numscalar(\"wald_r\",wald)\n",
      "\n",
      ". mat Ts[1,3]=wald             //save bootstrap Wald\n",
      "\n",
      ". \n",
      ". \n",
      ". \n"
     ]
    }
   ],
   "source": [
    "%%stata\n",
    "\n",
    "****************************************************\n",
    "*  Code for Table 2 of Kline and Santos (2011)     *\n",
    "*  This file computes Analytical Wald and LM tests *\n",
    "*  along with Score bootstrapped tests.   \t   *\n",
    "*  Some other results not reported in the paper    *\n",
    "*  are also included.\t\t\t\t   *\t\n",
    "****************************************************\n",
    "\n",
    "version 11.1\n",
    "set seed 12345\n",
    "\n",
    "**mata subroutine to compute recentered clustered outer products**\n",
    "**inputs a matrix S of scores and a cluster summing matrix L**\n",
    "**outputs clustered outer product matrix**\n",
    "**note this program assumes a balanced design**\n",
    "**and proper sort order**\n",
    "cap mata: mata drop clustOPG()\n",
    "mata: \n",
    "real matrix clustOPG(real matrix S,real matrix L){\n",
    "real scalar N\n",
    "real matrix Scent, Scentsum, OPGcent\n",
    "\n",
    "N=rows(S)\n",
    "Scent=S - J(N,1,1)*mean(S)\n",
    "Scentsum=L'*Scent\n",
    "OPGcent=Scentsum'*Scentsum\n",
    "return(OPGcent)\n",
    "}\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "/* cap prog drop sim_probit\n",
    "prog def sim_probit, rclass\n",
    "syntax [, c(integer 5) m(integer 50) f(integer 0) r(integer 199)] */\n",
    "*Notes on syntax: \n",
    "*c - # of clusters\n",
    "*m - # of obs per cluster\n",
    "*f - controls whether to use mixture regressor\n",
    "*r - # of bootstrap reps per simulation\n",
    "\n",
    "local c 10\n",
    "local m 100\n",
    "local f 0\n",
    "local r 1000\n",
    "\n",
    "**Generate data**\n",
    "drop _all\n",
    "set obs `c'  //generate c clusters\n",
    "\n",
    "gen x=invnorm(uniform()) //covariate\n",
    "gen mix=uniform()>.1 //contamination probability\n",
    "\n",
    "if `f'==1{\n",
    "\tgen D=(mix*invnorm(uniform()) + (1-mix)*(2+3*invnorm(uniform())))/4 //regressor of interest\n",
    "}\n",
    "else{\n",
    "\tgen D=(invnorm(uniform()))/4 //regressor of interest\n",
    "}\n",
    "\n",
    "gen v=invnorm(uniform())\n",
    "\n",
    "\n",
    "gen c=_n\t//generate cluster id\n",
    "\n",
    "expand `m'\t//make m obs per cluster\n",
    "\n",
    "replace x=(x+invnorm(uniform()))/4  //add some within cluster variation in x\n",
    "\n",
    "gen y=x+D+v/sqrt(2)+invnorm(uniform())/sqrt(2)  //form outcome\n",
    "replace y=y>0\n",
    "sort c\n",
    "\n",
    "**Restricted**\n",
    "constraint 1 D=1\n",
    "glm y x D, constraint(1) fam(bin) link(probit)\n",
    "predict pr\n",
    "predict xbr, xb\n",
    "gen phir=normalden(xbr)\n",
    "gen er=(y-pr)*phir/(pr*(1-pr)) //quasi-residual\n",
    "gen wr=phir^2/(pr*(1-pr)) //weights for hessian\n",
    "gen one=1\n",
    "\n",
    "mat accum Hr=one x D [iw=wr], nocons //form hessian */\n",
    "\n",
    "\n",
    "**Unrestricted**\n",
    "probit y x D, cluster(c)\n",
    "global b=_b[D]\n",
    "local stdof=sqrt((`m'*`c'-3)/(`m'*`c'-1)) //inverse stata DOF correction\n",
    "global W=((_b[D]-1)/(_se[D]*`stdof'))^2 //save analytical Wald\n",
    "\n",
    "predict pu\n",
    "predict xbu, xb\n",
    "gen phiu=normalden(xbu)\n",
    "gen eu=(y-pu)*phiu/(pu*(1-pu)) //quasi-residual\n",
    "gen wu=phiu^2/(pu*(1-pu)) //weights for hessian\n",
    "\n",
    "local dfk=(`c'/(`c'-1))\n",
    "mata: L=I(`c')#J(`m',1,1) //matrix for summing across clusters\n",
    "mata: X=st_data(.,(\"one\", \"x\", \"D\"))\n",
    "mata: C=(0\\0\\1)\t\t//constraint matrix -- third coefficient is restricted\n",
    "\n",
    "mata: Hr=st_matrix(\"Hr\")\n",
    "mata: Hrinv=invsym(Hr)\n",
    "mata: Ar=C'*Hrinv\t\t//form A_n\n",
    "\n",
    "\n",
    "**Prepare influence function wald\n",
    "mat accum Hu=one x D [iw=wu], nocons //form hessian\n",
    "mata: Hu=st_matrix(\"Hu\")\n",
    "mata: Huinv=invsym(Hu)\n",
    "mata: Au=C'*Huinv\n",
    "mata: eu=st_data(.,\"eu\")\n",
    "mata: OPGucent=clustOPG(X:*eu,L)\n",
    "mata: Vinvu=invsym(Au*OPGucent*`dfk'*Au') //compute inverse of variance of influence function\n",
    "\n",
    "\n",
    "save \"sim_sample.dta\", replace\n",
    "\n",
    "mata: S = X:*eu\n",
    "\n",
    "**Bootstrap**\n",
    "gen u=.\n",
    "gen ystar=.\n",
    "gen ernew=.\n",
    "gen eunew=.\n",
    "gen pos=.\n",
    "\n",
    "\n",
    "mat Ts=J(1,6,.) //store bootstrap statistics\n",
    "\n",
    "by c: replace u=uniform()\n",
    "by c: replace pos=u[1]<.5  //cluster level rademacher indicator\n",
    "gen radem = 2*pos - 1\n",
    "\n",
    "qui replace ernew=(2*pos-1)*er  //weighted residual\n",
    "qui replace eunew=(2*pos-1)*eu  //weighted residual\n",
    "\n",
    "*Score bootstrap LM -- Rademacher\n",
    "mata: er=st_data(.,\"ernew\")\n",
    "mata: Sr=colsum(X:*er)\n",
    "mata: OPGrcent=clustOPG(X:*er,L)\n",
    "mata: lm=Sr*Ar'*invsym(Ar*(OPGrcent)*`dfk'*Ar')*Ar*Sr'\n",
    "mata: st_numscalar(\"lm\",lm)\n",
    "mat Ts[1,1]=lm\t     //save bootstrap LM \n",
    "\n",
    "\n",
    "*Score Wald -- Rademacher\n",
    "mata: eu=st_data(.,\"eunew\")\n",
    "mata: Su=colsum(X:*eu)\n",
    "mata: OPGucent=clustOPG(X:*eu,L)\n",
    "mata: wald=Su*Au'*invsym(Au*OPGucent*`dfk'*Au')*Au*Su'\n",
    "mata: st_numscalar(\"wald\",wald)\n",
    "mat Ts[1,2]=wald\t     //save bootstrap Wald\n",
    "\n",
    "*Score bootstrap Wald combining restricted and unrestricted scores -- Rademacher\n",
    "mata: wald_r=Sr*Ar'*invsym(Au*OPGucent*`dfk'*Au')*Ar*Sr'\n",
    "mata: st_numscalar(\"wald_r\",wald)\n",
    "mat Ts[1,3]=wald\t     //save bootstrap Wald\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "rng = np.random.default_rng(seed=1)\n",
    "num_X = 2\n",
    "\n",
    "\n",
    "# generates fake probit data for testing (Not used in favor of data generated from paper)\n",
    "def probit_data(N, num_X):\n",
    "    # generate X and error\n",
    "    epsilon = rng.normal(size=N)\n",
    "    X = sm.add_constant(rng.normal(size=(N, num_X)))\n",
    "    beta = np.arange(1,num_X+2)\n",
    "    \n",
    "    true_y = X @ beta + epsilon\n",
    "    \n",
    "    y_star = np.where(true_y >0, 1,0)\n",
    "    \n",
    "    return y_star, true_y, X\n",
    "\n",
    "y_star, true_y, X = probit_data(1000, 3)\n",
    "\n",
    "clusters = rng.choice(range(100), size=1000)\n",
    "\n",
    "df = stata.pdataframe_from_data()\n",
    "\n",
    "p = Probit.from_formula(\"y ~ x + D\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.665335\n",
      "         Iterations 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Probit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>y</td>        <th>  No. Observations:  </th>  <td>  1000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                <td>Probit</td>      <th>  Df Residuals:      </th>  <td>   997</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     2</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Thu, 06 Jun 2024</td> <th>  Pseudo R-squ.:     </th>  <td>0.04010</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>19:23:11</td>     <th>  Log-Likelihood:    </th> <td> -665.33</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -693.13</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>cluster</td>     <th>  LLR p-value:       </th> <td>8.494e-13</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    0.0358</td> <td>    0.308</td> <td>    0.116</td> <td> 0.908</td> <td>   -0.568</td> <td>    0.640</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x</th>         <td>    1.0579</td> <td>    0.396</td> <td>    2.670</td> <td> 0.008</td> <td>    0.281</td> <td>    1.835</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>D</th>         <td>    0.0943</td> <td>    1.043</td> <td>    0.090</td> <td> 0.928</td> <td>   -1.950</td> <td>    2.139</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}   &        y         & \\textbf{  No. Observations:  } &     1000    \\\\\n",
       "\\textbf{Model:}           &      Probit      & \\textbf{  Df Residuals:      } &      997    \\\\\n",
       "\\textbf{Method:}          &       MLE        & \\textbf{  Df Model:          } &        2    \\\\\n",
       "\\textbf{Date:}            & Thu, 06 Jun 2024 & \\textbf{  Pseudo R-squ.:     } &  0.04010    \\\\\n",
       "\\textbf{Time:}            &     19:23:11     & \\textbf{  Log-Likelihood:    } &   -665.33   \\\\\n",
       "\\textbf{converged:}       &       True       & \\textbf{  LL-Null:           } &   -693.13   \\\\\n",
       "\\textbf{Covariance Type:} &     cluster      & \\textbf{  LLR p-value:       } & 8.494e-13   \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                   & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept} &       0.0358  &        0.308     &     0.116  &         0.908        &       -0.568    &        0.640     \\\\\n",
       "\\textbf{x}         &       1.0579  &        0.396     &     2.670  &         0.008        &        0.281    &        1.835     \\\\\n",
       "\\textbf{D}         &       0.0943  &        1.043     &     0.090  &         0.928        &       -1.950    &        2.139     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{Probit Regression Results}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                          Probit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:                 1000\n",
       "Model:                         Probit   Df Residuals:                      997\n",
       "Method:                           MLE   Df Model:                            2\n",
       "Date:                Thu, 06 Jun 2024   Pseudo R-squ.:                 0.04010\n",
       "Time:                        19:23:11   Log-Likelihood:                -665.33\n",
       "converged:                       True   LL-Null:                       -693.13\n",
       "Covariance Type:              cluster   LLR p-value:                 8.494e-13\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      0.0358      0.308      0.116      0.908      -0.568       0.640\n",
       "x              1.0579      0.396      2.670      0.008       0.281       1.835\n",
       "D              0.0943      1.043      0.090      0.928      -1.950       2.139\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.fit(cov_type='cluster', cov_kwds={'groups' : df['c']}).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.array(sfi.Mata.get(\"L\"))\n",
    "S = np.array(sfi.Mata.get(\"S\"))\n",
    "OPGucent = np.array(sfi.Mata.get(\"OPGucent\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.665335\n",
      "         Iterations 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Intercept    0.856059\n",
       "x            7.202267\n",
       "D            0.662808\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.fit().tvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, TypeVar\n",
    "import numpy.typing as npt\n",
    "from statsmodels.base.model import Model\n",
    "\n",
    "ArrayLike = Union[npt.ArrayLike, pd.DataFrame, pd.Series]\n",
    "\n",
    "class IncorrectModelException(Exception):\n",
    "    pass\n",
    "\n",
    "class ScoreWildBootstrap:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 mod: Model, \n",
    "                 clusters=Union[None, ArrayLike]) -> None:\n",
    "        \"\"\"Score wild bootstrap as in Kline and Santos (2012). Perturbs the score of using wild bootstrap weights and calculates wald statistics.\n",
    "        Optionally with restricted score or unrestricted scores.\n",
    "\n",
    "        Args:\n",
    "            mod (Model): A statsmodels model that has a `score_obs` and `hessian` method.\n",
    "            clusters (ArrayLike|None, optional): A dataframe or array of shape Nx1 that gives the cluster membership of each of the N observations. Defaults to Union[None, ArrayLike].\n",
    "        \"\"\"        \n",
    "        \n",
    "        if not hasattr(mod, \"score_obs\"):\n",
    "            raise IncorrectModelException(f\"{mod} does not implement `score_obs`.\")\n",
    "        if not hasattr(mod, \"hessian\"):\n",
    "            raise IncorrectModelException(\"{mod} does not implement `hessian`.\")\n",
    "        \n",
    "        self.mod = mod\n",
    "        \n",
    "        self.fitted_mod = self.mod.fit(cov_type='cluster', cov_kwds = {'groups' : clusters})\n",
    "        \n",
    "        self.beta_hat = self.fitted_mod.params\n",
    "        \n",
    "        if clusters is not None:\n",
    "            self.cluster_ids, self.cluster_obs = np.unique(clusters, return_counts=True)\n",
    "            self.clusters = clusters\n",
    "            self.num_clusters = len(self.cluster_ids)\n",
    "        else:\n",
    "            self.clusters = np.ones(self.mod.endog.shape[0])\n",
    "            self.cluster_ids = np.array([1])\n",
    "            self.num_clusters = 1\n",
    "            \n",
    "    def get_analytical_wald(self) -> pd.Series:\n",
    "        \"\"\"Calculates cluster-robust analytical wald test for each parameter.\n",
    "        \"\"\"        \n",
    "        return self.fitted_mod.tvalues**2\n",
    "\n",
    "    def wald_test(self, W: ArrayLike, restricted_residuals: bool =False, \n",
    "                  R: ArrayLike =None, r: Union[None, int, float]=None) -> Union[ArrayLike, float]:\n",
    "        \"\"\"Calculates Wald statistic, given perturbation weights, scores and hessians from the model.\n",
    "\n",
    "        Args:\n",
    "            W (ArrayLike): Weight vector used to perturb observation score\n",
    "            restricted_residuals (bool, optional): Whether to construct the test with the restricted score. Defaults to False.\n",
    "                \\begin{aligned}\n",
    "                    & T_{n, 1}^{\\star s} \\equiv\\left(R H_n^{-1} S_n^{\\star}\\left(\\hat{\\beta}_u\\right)\\right)^{\\prime}\\left(R H_n^{-1} \\sum_n^{\\star s}\\left(\\hat{\\beta}_u\\right) H_n^{-1} R^{\\prime}\\right)^{-1}\\left(R H_n^{-1} S_n^{\\star}\\left(\\hat{\\beta}_u\\right)\\right) \\\\\n",
    "                    & T_{n, 2}^{\\star s} \\equiv\\left(R H_n^{-1} S_n^{\\star}\\left(\\hat{\\beta}_r\\right)\\right)^{\\prime}\\left(R H_n^{-1} \\sum_n^{\\star s}\\left(\\hat{\\beta}_u\\right) H_n^{-1} R^{\\prime}\\right)^{-1}\\left(R H_n^{-1} S_n^{\\star}\\left(\\hat{\\beta}_r\\right)\\right)\n",
    "                \\end{aligned}\n",
    "                See Equations 50 and 51 from Kline and Santos (2012).\n",
    "            R (ArrayLike, optional): Restriction matrix for test. Defaults to None.\n",
    "            r (Union[None, int, float], optional): test restriction. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Union[ArrayLike, float]: A Wald Statistic based on the perturbed weights.\n",
    "        \"\"\"        \n",
    "        \n",
    "                        \n",
    "        hessian_inv = -np.linalg.inv(self.mod.hessian(self.beta_hat))\n",
    "        \n",
    "        if restricted_residuals: # if we use the restricted score in the test\n",
    "            if r is None:\n",
    "                raise ValueError(\"Must specify coefficient constraint to restrict residuals\")\n",
    "            score_beta = self.beta_hat.copy()\n",
    "            score_beta[np.where(R==1)[0]] = 1 \n",
    "        else:\n",
    "            score_beta = self.beta_hat\n",
    "        \n",
    "        # make array of weights divided by cluster membership\n",
    "        W_Jn = np.select([self.clusters == c for c in self.cluster_ids], W.flatten())[:,np.newaxis]\n",
    "        # divide weights by observations per clusters            \n",
    "        perturbed_score_obs = self.mod.score_obs(self.beta_hat) * W_Jn\n",
    "        \n",
    "        # multiply by cluster membership\n",
    "        L = pd.get_dummies(self.clusters).astype(int).values\n",
    "        \n",
    "        perturbed_score_obs_mean = (((L/(L.sum(axis=0)-1)).T @ perturbed_score_obs)).mean(axis=0)\n",
    "        \n",
    "        demeaned_perturbed_score_obs = perturbed_score_obs - perturbed_score_obs_mean \n",
    "        \n",
    "        demeaned_perturbed_score_obs_L = L.T @ demeaned_perturbed_score_obs\n",
    "        \n",
    "        perturbed_score = (self.mod.score_obs(score_beta) * W_Jn).sum(axis=0)\n",
    "        \n",
    "        # equation 50 from the paper\n",
    "        bread = R @ hessian_inv @ perturbed_score\n",
    "        \n",
    "        opg_centered = demeaned_perturbed_score_obs_L.T @ demeaned_perturbed_score_obs_L\n",
    "        \n",
    "        meat = R @ hessian_inv @ opg_centered @ hessian_inv @ R.T\n",
    "        \n",
    "        if isinstance(bread, np.ndarray):\n",
    "            meat_inv = np.linalg.inv(meat)\n",
    "            return bread.T @ meat_inv @ bread\n",
    "        else:\n",
    "            return (bread)**2/meat \n",
    "        \n",
    "    def bootstrap(self, R: ArrayLike, \n",
    "                  W: Union[ArrayLike,None]=None, \n",
    "                  r: Union[int, float, None]=None, \n",
    "                  n_jobs: int = -1, \n",
    "                  rng: Union[np.random.Generator, None] = None, \n",
    "                  seed: Union[int, None] = None, \n",
    "                  bootstrap_num: int =100, \n",
    "                  restricted_residuals: bool=False, \n",
    "                  full_enumeration: bool=False) -> ArrayLike:\n",
    "        \"\"\"Bootstraps Wald Statistics by generating weights and continuously perturbing the score\n",
    "\n",
    "        Args:\n",
    "            R (ArrayLike, optional): Restriction Matrix. Defaults to None.\n",
    "            r (Union[int, float], optional): restriction. Defaults to None.\n",
    "            W (ArrayLike, optional): Weights vector. If not specified, Rademacher weights will be generated. Defaults to None.\n",
    "            n_jobs (int, optional): Number of jobs to dispath for parallel processing. Defaults to -1, or all cores.\n",
    "            rng (Union[np.random.Generator, None], optional): random generator that can be used for reproducibility. Defaults to None.\n",
    "            seed (Union[int, None], optional): seed for random generator. Defaults to None.\n",
    "            bootstrap_num (int, optional): number of bootstraps to perform. Defaults to 100.\n",
    "            restricted_residuals (bool, optional): Whether to use restricted score in constructing Wald. Defaults to False.\n",
    "            full_enumeration (bool, optional): Whether to generate the full set of random weight possibilities. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            ArrayLike: A `bootstrap_num` x 1 matrix.\n",
    "        \"\"\"        \n",
    "        \n",
    "        def bootstrapper(restricted_residuals, R, r, full_enumeration, rng, W):\n",
    "            if rng is None:\n",
    "                rng=np.random.RandomState(seed)\n",
    "\n",
    "            if W is None:\n",
    "                W = draw_weights('rademacher',\n",
    "                        full_enumeration=full_enumeration,\n",
    "                        N_G_bootcluster=self.num_clusters,\n",
    "                        boot_iter=1,\n",
    "                        rng=rng)[0]\n",
    "            \n",
    "            return self.wald_test(W = W, restricted_residuals=restricted_residuals, R=R, r=r)\n",
    "        \n",
    "        walds = Parallel(n_jobs=n_jobs, backend='loky')(delayed(bootstrapper)(restricted_residuals=restricted_residuals, \n",
    "                                                             R=R, r=r, full_enumeration=full_enumeration, rng=rng, W=W) for b in range(bootstrap_num))\n",
    "        \n",
    "        return np.array(walds)\n",
    "    \n",
    "    \n",
    "    def get_wald_boot(self, n_jobs: int =-1, \n",
    "                      bootstrap_num: int =100, \n",
    "                      restricted_residuals: bool=False, \n",
    "                      full_enumeration: bool=False, \n",
    "                      rng: Union[np.random.Generator, None] =None, \n",
    "                      seed: Union[int, None]=None,\n",
    "                      W= Union[ArrayLike,None]) -> ArrayLike:\n",
    "        \"\"\"Generates Wald Statistics for each parameter separately.\n",
    "\n",
    "        Args:\n",
    "            n_jobs (int): Number of jobs to dispath for parallel processing. Defaults to -1, or all cores.\n",
    "            bootstrap_num (int, optional): number of bootstraps to perform. Defaults to 100.\n",
    "            restricted_residuals (bool, optional): Whether to use restricted score in constructing Wald. Defaults to False.\n",
    "            full_enumeration (bool, optional): Whether to generate the full set of random weight possibilities. Defaults to False.\n",
    "            rng (Union[np.random.Generator, None], optional): random generator that can be used for reproducibility. Defaults to None.\n",
    "            seed (Union[int, None], optional): seed for random generator. Defaults to None.\n",
    "            W ( Union[ArrayLike,None], optional): Weights vector. If not specified, Rademacher weights will be generated. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            ArrayLike: A `bootstrap_num` x k matrix where k is the number of parameters.\n",
    "        \"\"\"        \n",
    "        \n",
    "        # generate restriction vectors for each variable for beta=0\n",
    "        R = np.identity(self.beta_hat.shape[0])\n",
    "                \n",
    "        wald_boot = np.zeros(shape=(bootstrap_num, self.beta_hat.shape[0]))\n",
    "        \n",
    "        for i, R_vec in enumerate(R):\n",
    "            wald_boot[:, i] = self.bootstrap(R = R_vec, r = 0, n_jobs=n_jobs,\n",
    "                                          rng=rng, seed=seed, bootstrap_num=bootstrap_num,\n",
    "                                          restricted_residuals=restricted_residuals,\n",
    "                                          full_enumeration=full_enumeration,\n",
    "                                          W=None)\n",
    "            \n",
    "        return wald_boot\n",
    "        \n",
    "        \n",
    "    def get_pvalue(self, pval_type: str='two-tailed', **kwargs) -> ArrayLike:\n",
    "        \"\"\"Generates p-values by comparing analytical wald statistics with boostrapped, perturbed Wald statistics\n",
    "\n",
    "        Args:\n",
    "            pval_type (str, optional): The type of p-value to generate. Defaults to 'two-tailed'.\n",
    "            kwargs can are used for specifying other options for `get_wald_boot`.\n",
    "\n",
    "        Returns:\n",
    "            ArrayLike: An array of p-values for each parameter\n",
    "        \"\"\"        \n",
    "        \n",
    "        wald_boot = self.get_wald_boot(**kwargs)\n",
    "        \n",
    "        analytical_wald = self.get_analytical_wald()\n",
    "        \n",
    "        if pval_type == \"two-tailed\":\n",
    "            pvalue = np.where(wald_boot > analytical_wald.values, 1, 0).mean(axis=0)\n",
    "        elif pval_type == \"equal-tailed\":\n",
    "            pl = np.where(analytical_wald.values < wald_boot, 1,0).mean(axis=0)\n",
    "            ph = np.where(analytical_wald.values > wald_boot, 1,0).mean(axis=0)\n",
    "            pvalue = 2 * min(pl, ph)\n",
    "        elif pval_type == \">\":\n",
    "            pvalue = np.where(analytical_wald.values < wald_boot,1,0).mean(axis=0)\n",
    "        else:\n",
    "            pvalue = np.where(analytical_wald.values > wald_boot,1,0).mean(axis=0)\n",
    "            \n",
    "        return pvalue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.665335\n",
      "         Iterations 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.909, 0.028, 0.931])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb = ScoreWildBootstrap(p, clusters=df['c'])\n",
    "\n",
    "ts = sb.get_pvalue(n_jobs=1, bootstrap_num=1000)\n",
    "\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_obs(self, params):\n",
    "    X= self.exog\n",
    "    y = self.endog\n",
    "    return (X.T*(y - X @ params)).T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.OLS.score_obs = score_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_mod = sm.OLS(true_y, np.column_stack([X, rng.normal(size=1000)]))\n",
    "\n",
    "linear_mod = sm.OLS.from_formula(\"y ~ x +D\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| param     |   statistic |   p-value |\n",
      "|:----------|------------:|----------:|\n",
      "| Intercept |      32.953 |     0.000 |\n",
      "| x         |       7.820 |     0.000 |\n",
      "| D         |       0.632 |     0.526 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lordflaron/Documents/wildboottest/.venv/lib/python3.9/site-packages/tabulate/__init__.py:107: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  (len(row) >= 1 and row[0] == SEPARATING_LINE)\n",
      "/Users/lordflaron/Documents/wildboottest/.venv/lib/python3.9/site-packages/tabulate/__init__.py:108: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  or (len(row) >= 2 and row[1] == SEPARATING_LINE)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statistic</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Intercept</th>\n",
       "      <td>[32.95279531027586]</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>[7.820060460734793]</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>[0.6320615764783465]</td>\n",
       "      <td>0.526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      statistic  p-value\n",
       "param                                   \n",
       "Intercept   [32.95279531027586]    0.000\n",
       "x           [7.820060460734793]    0.000\n",
       "D          [0.6320615764783465]    0.526"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wildboottest(linear_mod, B=1000, weights_type='rademacher',\n",
    "                impose_null=True, bootstrap_type='11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.   , 0.001, 0.868])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb = ScoreWildBootstrap(linear_mod, clusters=df['c'])\n",
    "\n",
    "ts = sb.get_pvalue(n_jobs=1, bootstrap_num=1000)\n",
    "\n",
    "ts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
